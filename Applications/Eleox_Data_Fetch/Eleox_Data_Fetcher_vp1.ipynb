{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Helper Functions --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_path():\n",
    "    return Path.home() / \"Downloads\"\n",
    "\n",
    "def files_in_folder(path,file_ext = \"\"):\n",
    "    return [k for k in Path(path).iterdir() if k.is_file() \n",
    "                if file_ext in k.suffix]\n",
    "\n",
    "def download_wait(\n",
    "    directory, \n",
    "    timeout=100000, \n",
    "    nfiles=None,\n",
    "    verbose = False,\n",
    "    verbose_while_waiting = False,):\n",
    "    \"\"\"\n",
    "    Wait for downloads to finish with a specified timeout.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    directory : str\n",
    "        The path to the folder where the files will be downloaded.\n",
    "    timeout : int\n",
    "        How many seconds to wait until timing out.\n",
    "    nfiles : int, defaults to None\n",
    "        If provided, also wait for the expected number of files.\n",
    "\n",
    "    \"\"\"\n",
    "    st = time.time()\n",
    "    seconds = 0\n",
    "    dl_wait = True\n",
    "    while dl_wait and seconds < timeout:\n",
    "        if verbose_while_waiting:\n",
    "            print(f\"Waiting for Download\")\n",
    "        time.sleep(1)\n",
    "        dl_wait = False\n",
    "        curr_files = files_in_folder(directory)\n",
    "        if nfiles and len(curr_files) != nfiles:\n",
    "            dl_wait = True\n",
    "\n",
    "        for fname in curr_files:\n",
    "            if str(fname.absolute()).endswith('.crdownload'):\n",
    "                dl_wait = True\n",
    "\n",
    "        seconds += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        print(f\"Total time for download wait = {time.time() - st}\")\n",
    "    return seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Data Retrieval Functions --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\celii\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_categories = (\"Interstate\",\"Other\",\"Midstream\")\n",
    "default_categories = \"Other\"\n",
    "default_base_url = \"https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=ARLS\"\n",
    "default_category_class_name = \"igdm_NautilusMenuItemHorizontalRootLink\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_download_links(\n",
    "    categories = None,\n",
    "    base_url = None,\n",
    "    category_class_name=None,\n",
    "    query_deliminiter = \"?\",\n",
    "    return_df = False,\n",
    "    verbose = True,\n",
    "    **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Purpose: Retrieve the URLS for data to be downloaded along with the names and categories.\n",
    "    \n",
    "    Implementation: beautifulsoup web scraping\n",
    "    \"\"\"\n",
    "    \n",
    "    if categories is None:\n",
    "        categories = default_categories\n",
    "        \n",
    "    if type(categories) == str:\n",
    "        categories = [categories]\n",
    "        \n",
    "    if base_url is None:\n",
    "        base_url = default_base_url\n",
    "        \n",
    "    if category_class_name is None:\n",
    "        category_class_name=default_category_class_name\n",
    "        \n",
    "    \n",
    "    \n",
    "    # -- downloads the html of the base page--\n",
    "    download_base = base_url[:base_url.find(query_deliminiter)]\n",
    "    page = requests.get(base_url)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "    # -- finds all the data categories on base page\n",
    "    categories_total = soup.find_all(class_=category_class_name)\n",
    "\n",
    "    link_dicts = []\n",
    "    for cat in categories_total:\n",
    "        \n",
    "            \n",
    "        cat_name = cat.span.text\n",
    "        \n",
    "        if cat_name not in categories:\n",
    "            continue\n",
    "\n",
    "        a_links = cat.next_sibling.find_all(\"a\")\n",
    "\n",
    "        local_dicts = [dict(\n",
    "            download_link = f'{download_base}?{a[\"href\"][a[\"href\"].find(query_deliminiter)+1:].replace(\"TSP=\",\"code=\")}',\n",
    "            name = a.span.text,\n",
    "            category = cat_name) for a in a_links]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"# of links in {cat_name} = {len(local_dicts)}\")\n",
    "\n",
    "        link_dicts += local_dicts\n",
    "\n",
    "    if return_df:\n",
    "        return pd.DataFrame.from_records(link_dicts)\n",
    "    else:\n",
    "        return link_dicts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_driver_exe_path = str((download_path() / Path(\"chromedriver_win32/chromedriver.exe\")).absolute())\n",
    "default_retrieve_button_id = \"WebSplitter1_tmpl1_ContentPlaceHolder1_HeaderBTN1_btnRetrieve\"\n",
    "default_download_button_id = \"WebSplitter1_tmpl1_ContentPlaceHolder1_HeaderBTN1_btnDownload\"\n",
    "default_visible_browser = True\n",
    "default_append_source = True\n",
    "default_retrieve_sleep_seconds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(\n",
    "    data_to_download,\n",
    "    driver_exe_path = None,\n",
    "    retrieve_button_id = None,\n",
    "    download_button_id = None,\n",
    "    visible_browser = None,\n",
    "    append_source = None,\n",
    "    retrieve_sleep_seconds = None,\n",
    "    ignore_empty_download = True,\n",
    "    debug = False,\n",
    "    verbose = True,\n",
    "    **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Purpose: To go and fetch the data from the\n",
    "    download links and to export as a csv file\n",
    "    \"\"\"\n",
    "    \n",
    "    if driver_exe_path is None:\n",
    "        driver_exe_path = default_driver_exe_path\n",
    "        \n",
    "    if retrieve_button_id is None:\n",
    "        retrieve_button_id = default_retrieve_button_id\n",
    "        \n",
    "    if download_button_id is None:\n",
    "        download_button_id = default_download_button_id\n",
    "        \n",
    "    if visible_browser is None:\n",
    "        visible_browser = default_visible_browser\n",
    "        \n",
    "    if append_source is None:\n",
    "        append_source = default_append_source\n",
    "        \n",
    "    if retrieve_sleep_seconds is None:\n",
    "        retrieve_sleep_seconds = default_retrieve_sleep_seconds\n",
    "        \n",
    "    \n",
    "    #1) Creates the Chrome browser\n",
    "    if not visible_browser:\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")\n",
    "        assert options.headless\n",
    "    else:\n",
    "        options = None\n",
    "        \n",
    "    \n",
    "    curr_path = str(Path(driver_exe_path).absolute())\n",
    "    if debug:\n",
    "        print(f\"Path to Chrome driver = {curr_path}\")\n",
    "\n",
    "    driver = Chrome(curr_path,options = options)\n",
    "    \n",
    "    #2) The data retrieval loop\n",
    "    \"\"\"\n",
    "    Purpose: Given a link where data can be found: \n",
    "    1) Go to webpage\n",
    "    2) click the retrieve button\n",
    "    3) click the download button\n",
    "    4) wait for the download\n",
    "    5) Load the file as a dataframe and delete from downloads\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(data_to_download,pd.DataFrame,):\n",
    "        data_to_download = data_to_download.to_dict(orient='records')\n",
    "        \n",
    "    all_link_dfs = []\n",
    "    for data_dict in data_to_download:\n",
    "        data_link = data_dict[\"download_link\"]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n-- Working on downloading {data_dict['category']}:{data_dict['name']} from \\n {data_dict['download_link']}\")\n",
    "\n",
    "        driver.get(data_link)\n",
    "\n",
    "        elem = driver.find_element(By.ID,retrieve_button_id)\n",
    "        elem.click()\n",
    "\n",
    "        time.sleep(retrieve_sleep_seconds)\n",
    "\n",
    "        downloads = download_path()\n",
    "        download_files = files_in_folder(downloads)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"# of download files before download = {len(download_files)}\")\n",
    "\n",
    "        elem = driver.find_element(By.ID,download_button_id)\n",
    "        elem.click()\n",
    "\n",
    "        download_wait(\n",
    "            downloads,\n",
    "            nfiles = len(download_files) + 1,\n",
    "            verbose = False,\n",
    "            verbose_while_waiting = False)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"# of download files after download = {len(download_files)}\")\n",
    "\n",
    "        download_files_after = files_in_folder(downloads)\n",
    "        file = list(set(download_files_after).difference(download_files))[0]\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "        except:\n",
    "            df = pd.read_excel(file)\n",
    "\n",
    "        if not ignore_empty_download and len(df) == 0:\n",
    "            raise Exception(f\"The following data fetch had 0 entries: \\n{data_dict}\")\n",
    "\n",
    "        if append_source:\n",
    "            for k,v in data_dict.items():\n",
    "                df[k] = v\n",
    "\n",
    "        file.unlink()\n",
    "        all_link_dfs.append(df)\n",
    "\n",
    "    all_link_dfs = pd.concat(all_link_dfs)\n",
    "    \n",
    "    return all_link_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_export_filepath = \"./download_data\"\n",
    "\n",
    "def export_data_df_to_csv(\n",
    "    data_df,\n",
    "    export_filepath=None,\n",
    "    verbose = True,\n",
    "    **kwargs\n",
    "    ):\n",
    "    \n",
    "    if export_filepath is None:\n",
    "        export_filepath = default_export_filepath\n",
    "        \n",
    "    export_path = Path(export_filepath)\n",
    "    export_path = export_path.parent / Path(f\"{export_path.stem}.csv\")\n",
    "    data_df.to_csv(str(export_path.absolute()))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Exportted to {export_path}\")\n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(**kwargs):\n",
    "            \n",
    "    data_links = fetch_download_links(**kwargs)\n",
    "    data_df = download_data(data_links,**kwargs)\n",
    "    export_data_df_to_csv(data_df,**kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    #-- arguemtns for output\n",
    "    parser.add_argument(\"-f\",\"--export_filepath\",default=None,\n",
    "                        help=\"csv filepath for the output of data\",\n",
    "                       dest = \"export_filepath\")\n",
    "    # -- arguments for downloading --\n",
    "    parser.add_argument(\"-d\",\"--driver_exe_path\",default=default_driver_exe_path,\n",
    "                        help=\"path to where stored chrome driver exe\",\n",
    "                       dest = \"driver_exe_path\")\n",
    "    parser.add_argument(\"-ret\",\"--retrieve_button_id\",default=default_retrieve_button_id,help=\"the id of the retrieve button in the html source\",\n",
    "                       dest = \"retrieve_button_id\")\n",
    "    parser.add_argument(\"-down\",\"--download_button_id\",default = default_download_button_id,help=\"the id of the download button in the html source\",\n",
    "                       dest = \"download_button_id\")\n",
    "    parser.add_argument(\"-v\",\"--visible_browser\",default = default_visible_browser,\n",
    "                        help=\"whether a browser window should pop up and perform the scripted actions. Set to False for headerless\",\n",
    "                       dest = \"visible_browser\")\n",
    "    parser.add_argument(\"-a\",\"--append_source\",default=default_append_source,\n",
    "                       help=\"whether the url and the pipeline name should be appended to the entries to show where entry was fetched from\",\n",
    "                       dest = \"append_source\")\n",
    "    parser.add_argument(\"-s\",\"--retrieve_sleep_seconds\",default = default_retrieve_sleep_seconds,\n",
    "                       help=\"how long the program will sleep after activating the retrieve button (to help if takes long time to buffer)\",\n",
    "                       dest = \"retrieve_sleep_seconds\")\n",
    "    \n",
    "    # -- arguments for fetch_download_links --\n",
    "    parser.add_argument(\"-b\",\"--base_url\",default = default_base_url,\n",
    "                        help = \"what webpage to start from\",\n",
    "                       dest = \"base_url\")\n",
    "    parser.add_argument(\"-cat_n\",\"--category_class_name\",default = default_category_class_name,\n",
    "                        help = \"the class name from the html source to which signal which tags to search for in finding categories\",\n",
    "                       dest = \"category_class_name\")\n",
    "    parser.add_argument(\"-cat\",\"--categories\",default=default_categories,\n",
    "                        help=(\"the pipelines to pull data from (listed in the dropdown tabs of webpage). \"\n",
    "                        \"Currently only supports one pipeline input specified with str\"),\n",
    "                        type = str,\n",
    "                        nargs='+',\n",
    "                       dest = \"categories\")\n",
    "    \n",
    "    \n",
    "    # to run in ipynb\n",
    "    args = parser.parse_args(\"-f download.csv\".split())\n",
    "    \n",
    "    # to run as script\n",
    "    #args = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    return args\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments = Namespace(append_source=True, base_url='https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=ARLS', categories='Other', category_class_name='igdm_NautilusMenuItemHorizontalRootLink', download_button_id='WebSplitter1_tmpl1_ContentPlaceHolder1_HeaderBTN1_btnDownload', driver_exe_path='C:\\\\Users\\\\celii\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe', export_filepath='download.csv', retrieve_button_id='WebSplitter1_tmpl1_ContentPlaceHolder1_HeaderBTN1_btnRetrieve', retrieve_sleep_seconds=2, visible_browser=True)\n",
      "# of links in Other = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\celii\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:51: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Working on downloading Other:Keystone Gas Storage from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=KGS\n",
      "\n",
      "-- Working on downloading Other:Twin Tier Pipeline from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=TTP\n",
      "Exportted to download.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    arguments = parse_arguments()\n",
    "    print(f\"arguments = {arguments}\")\n",
    "    main(**vars(arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
