{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Helper Functions --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_path():\n",
    "    return Path.home() / \"Downloads\"\n",
    "\n",
    "def files_in_folder(path,file_ext = \"\"):\n",
    "    return [k for k in Path(path).iterdir() if k.is_file() \n",
    "                if file_ext in k.suffix]\n",
    "\n",
    "def download_wait(\n",
    "    directory, \n",
    "    timeout=100000, \n",
    "    nfiles=None,\n",
    "    verbose = False,\n",
    "    verbose_while_waiting = False,):\n",
    "    \"\"\"\n",
    "    Wait for downloads to finish with a specified timeout.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    directory : str\n",
    "        The path to the folder where the files will be downloaded.\n",
    "    timeout : int\n",
    "        How many seconds to wait until timing out.\n",
    "    nfiles : int, defaults to None\n",
    "        If provided, also wait for the expected number of files.\n",
    "\n",
    "    \"\"\"\n",
    "    st = time.time()\n",
    "    seconds = 0\n",
    "    dl_wait = True\n",
    "    while dl_wait and seconds < timeout:\n",
    "        if verbose_while_waiting:\n",
    "            print(f\"Waiting for Download\")\n",
    "        time.sleep(1)\n",
    "        dl_wait = False\n",
    "        curr_files = files_in_folder(directory)\n",
    "        if nfiles and len(curr_files) != nfiles:\n",
    "            dl_wait = True\n",
    "\n",
    "        for fname in curr_files:\n",
    "            if str(fname.absolute()).endswith('.crdownload'):\n",
    "                dl_wait = True\n",
    "\n",
    "        seconds += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        print(f\"Total time for download wait = {time.time() - st}\")\n",
    "    return seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Data Retrieval Functions --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\celii\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_categories = (\"Interstate\",\"Other\",\"Midstream\")\n",
    "default_categories = \"Interstate\"\n",
    "default_base_url = \"https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=ARLS\"\n",
    "default_category_class_name = \"igdm_NautilusMenuItemHorizontalRootLink\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_download_links(\n",
    "    categories = None,\n",
    "    base_url = None,\n",
    "    category_class_name=None,\n",
    "    query_deliminiter = \"?\",\n",
    "    return_df = False,\n",
    "    verbose = True,\n",
    "    **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Purpose: Retrieve the URLS for data to be downloaded along with the names and categories.\n",
    "    \n",
    "    Implementation: beautifulsoup web scraping\n",
    "    \"\"\"\n",
    "    \n",
    "    if categories is None:\n",
    "        categories = default_categories\n",
    "        \n",
    "    if type(categories) == str:\n",
    "        categories = [categories]\n",
    "        \n",
    "    if base_url is None:\n",
    "        base_url = default_base_url\n",
    "        \n",
    "    if category_class_name is None:\n",
    "        category_class_name=default_category_class_name\n",
    "        \n",
    "    \n",
    "    \n",
    "    # -- downloads the html of the base page--\n",
    "    download_base = base_url[:base_url.find(query_deliminiter)]\n",
    "    page = requests.get(base_url)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "    # -- finds all the data categories on base page\n",
    "    categories_total = soup.find_all(class_=category_class_name)\n",
    "\n",
    "    link_dicts = []\n",
    "    for cat in categories_total:\n",
    "        \n",
    "            \n",
    "        cat_name = cat.span.text\n",
    "        \n",
    "        if cat_name not in categories:\n",
    "            continue\n",
    "\n",
    "        a_links = cat.next_sibling.find_all(\"a\")\n",
    "\n",
    "        local_dicts = [dict(\n",
    "            download_link = f'{download_base}?{a[\"href\"][a[\"href\"].find(query_deliminiter)+1:].replace(\"TSP=\",\"code=\")}',\n",
    "            name = a.span.text,\n",
    "            category = cat_name) for a in a_links]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"# of links in {cat_name} = {len(local_dicts)}\")\n",
    "\n",
    "        link_dicts += local_dicts\n",
    "\n",
    "    if return_df:\n",
    "        return pd.DataFrame.from_records(link_dicts)\n",
    "    else:\n",
    "        return link_dicts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_driver_exe_path = str((download_path() / Path(\"chromedriver_win32/chromedriver.exe\")).absolute())\n",
    "default_retrieve_button_id = \"WebSplitter1_tmpl1_ContentPlaceHolder1_HeaderBTN1_btnRetrieve\"\n",
    "default_download_button_id = \"WebSplitter1_tmpl1_ContentPlaceHolder1_HeaderBTN1_btnDownload\"\n",
    "default_visible_browser = True\n",
    "default_append_source = True\n",
    "default_retrieve_sleep_seconds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"./\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(\n",
    "    data_to_download,\n",
    "    driver_exe_path = None,\n",
    "    retrieve_button_id = None,\n",
    "    download_button_id = None,\n",
    "    visible_browser = None,\n",
    "    append_source = None,\n",
    "    retrieve_sleep_seconds = None,\n",
    "    ignore_empty_download = True,\n",
    "    debug = False,\n",
    "    verbose = True,\n",
    "    **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Purpose: To go and fetch the data from the\n",
    "    download links and to export as a csv file\n",
    "    \"\"\"\n",
    "    \n",
    "    if driver_exe_path is None:\n",
    "        driver_exe_path = default_driver_exe_path\n",
    "        \n",
    "    if retrieve_button_id is None:\n",
    "        retrieve_button_id = default_retrieve_button_id\n",
    "        \n",
    "    if download_button_id is None:\n",
    "        download_button_id = default_download_button_id\n",
    "        \n",
    "    if visible_browser is None:\n",
    "        visible_browser = default_visible_browser\n",
    "        \n",
    "    if append_source is None:\n",
    "        append_source = default_append_source\n",
    "        \n",
    "    if retrieve_sleep_seconds is None:\n",
    "        retrieve_sleep_seconds = default_retrieve_sleep_seconds\n",
    "        \n",
    "    \n",
    "    #1) Creates the Chrome browser\n",
    "    if not visible_browser:\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")\n",
    "        assert options.headless\n",
    "    else:\n",
    "        options = None\n",
    "        \n",
    "    \n",
    "    if not Path(driver_exe_path).exists():\n",
    "        guess_path = Path(f\"{driver_exe_path}\")\n",
    "        if '.exe' not in driver_exe_path:\n",
    "            guess_path = guess_path / Path(\"chromedriver.exe\")\n",
    "        driver_exe_path = str((download_path() / guess_path).absolute())\n",
    "        if verbose:\n",
    "            print(f\"Inferred path to driver = {driver_exe_path}\")\n",
    "    \n",
    "    curr_path = str(Path(driver_exe_path).absolute())\n",
    "    if debug:\n",
    "        print(f\"Path to Chrome driver = {curr_path}\")\n",
    "\n",
    "    driver = Chrome(curr_path,options = options)\n",
    "    \n",
    "    #2) The data retrieval loop\n",
    "    \"\"\"\n",
    "    Purpose: Given a link where data can be found: \n",
    "    1) Go to webpage\n",
    "    2) click the retrieve button\n",
    "    3) click the download button\n",
    "    4) wait for the download\n",
    "    5) Load the file as a dataframe and delete from downloads\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(data_to_download,pd.DataFrame,):\n",
    "        data_to_download = data_to_download.to_dict(orient='records')\n",
    "        \n",
    "    all_link_dfs = []\n",
    "    for data_dict in data_to_download:\n",
    "        data_link = data_dict[\"download_link\"]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n-- Working on downloading {data_dict['category']}:{data_dict['name']} from \\n {data_dict['download_link']}\")\n",
    "\n",
    "        driver.get(data_link)\n",
    "\n",
    "        elem = driver.find_element(By.ID,retrieve_button_id)\n",
    "        elem.click()\n",
    "\n",
    "        time.sleep(retrieve_sleep_seconds)\n",
    "\n",
    "        downloads = download_path()\n",
    "        download_files = files_in_folder(downloads)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"# of download files before download = {len(download_files)}\")\n",
    "\n",
    "        elem = driver.find_element(By.ID,download_button_id)\n",
    "        elem.click()\n",
    "\n",
    "        download_wait(\n",
    "            downloads,\n",
    "            nfiles = len(download_files) + 1,\n",
    "            verbose = False,\n",
    "            verbose_while_waiting = False)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"# of download files after download = {len(download_files)}\")\n",
    "\n",
    "        download_files_after = files_in_folder(downloads)\n",
    "        file = list(set(download_files_after).difference(download_files))[0]\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "        except:\n",
    "            df = pd.read_excel(file)\n",
    "\n",
    "        if not ignore_empty_download and len(df) == 0:\n",
    "            raise Exception(f\"The following data fetch had 0 entries: \\n{data_dict}\")\n",
    "\n",
    "        if append_source:\n",
    "            for k,v in data_dict.items():\n",
    "                df[k] = v\n",
    "\n",
    "        file.unlink()\n",
    "        all_link_dfs.append(df)\n",
    "\n",
    "    all_link_dfs = pd.concat(all_link_dfs)\n",
    "    \n",
    "    return all_link_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_export_filepath = \"./download_data\"\n",
    "\n",
    "def export_data_df_to_csv(\n",
    "    data_df,\n",
    "    export_filepath=None,\n",
    "    verbose = True,\n",
    "    **kwargs\n",
    "    ):\n",
    "    \n",
    "    if export_filepath is None:\n",
    "        export_filepath = default_export_filepath\n",
    "        \n",
    "    export_path = Path(export_filepath)\n",
    "    export_path = export_path.parent / Path(f\"{export_path.stem}.csv\")\n",
    "    data_df.to_csv(str(export_path.absolute()))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Exportted to {export_path}\")\n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fetch_pipeline(**kwargs):\n",
    "    data_links = fetch_download_links(**kwargs)\n",
    "    data_df = download_data(data_links,**kwargs)\n",
    "    export_data_df_to_csv(data_df,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    #-- arguemtns for output\n",
    "    parser.add_argument(\"-f\",\"--export_filepath\",default=None,\n",
    "                        help=\"csv filepath for the output of data\",\n",
    "                       dest = \"export_filepath\")\n",
    "    # -- arguments for downloading --\n",
    "    parser.add_argument(\"-d\",\"--driver_exe_path\",default=default_driver_exe_path,\n",
    "                        help=\"path to where stored chrome driver exe\",\n",
    "                       dest = \"driver_exe_path\")\n",
    "    parser.add_argument(\"-ret\",\"--retrieve_button_id\",default=default_retrieve_button_id,help=\"the id of the retrieve button in the html source\",\n",
    "                       dest = \"retrieve_button_id\")\n",
    "    parser.add_argument(\"-down\",\"--download_button_id\",default = default_download_button_id,help=\"the id of the download button in the html source\",\n",
    "                       dest = \"download_button_id\")\n",
    "    parser.add_argument(\"-v\",\"--visible_browser\",default = default_visible_browser,\n",
    "                        help=\"whether a browser window should pop up and perform the scripted actions. Set to False for headerless\",\n",
    "                       dest = \"visible_browser\")\n",
    "    parser.add_argument(\"-a\",\"--append_source\",default=default_append_source,\n",
    "                       help=\"whether the url and the pipeline name should be appended to the entries to show where entry was fetched from\",\n",
    "                       dest = \"append_source\")\n",
    "    parser.add_argument(\"-s\",\"--retrieve_sleep_seconds\",default = default_retrieve_sleep_seconds,\n",
    "                       help=\"how long the program will sleep after activating the retrieve button (to help if takes long time to buffer)\",\n",
    "                       dest = \"retrieve_sleep_seconds\")\n",
    "    \n",
    "    # -- arguments for fetch_download_links --\n",
    "    parser.add_argument(\"-b\",\"--base_url\",default = default_base_url,\n",
    "                        help = \"what webpage to start from\",\n",
    "                       dest = \"base_url\")\n",
    "    parser.add_argument(\"-cat_n\",\"--category_class_name\",default = default_category_class_name,\n",
    "                        help = \"the class name from the html source to which signal which tags to search for in finding categories\",\n",
    "                       dest = \"category_class_name\")\n",
    "    parser.add_argument(\"-cat\",\"--categories\",default=default_categories,\n",
    "                        help=(\"the pipelines to pull data from (listed in the dropdown tabs of webpage). \"\n",
    "                        \"Currently only supports one pipeline input specified with str\"),\n",
    "                        type = str,\n",
    "                        nargs='+',\n",
    "                       dest = \"categories\")\n",
    "    \n",
    "    \n",
    "    # to run in ipynb\n",
    "    #args = parser.parse_args(\"-f download.csv -d chromedriver_win32\".split())\n",
    "    \n",
    "    # to run as script\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    \n",
    "    return args\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    arguments = parse_arguments()\n",
    "    print(f\"arguments = {arguments}\")\n",
    "    kwargs = vars(arguments)\n",
    "    data_fetch_pipeline(**kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-f EXPORT_FILEPATH] [-d DRIVER_EXE_PATH]\n",
      "                             [-ret RETRIEVE_BUTTON_ID]\n",
      "                             [-down DOWNLOAD_BUTTON_ID] [-v VISIBLE_BROWSER]\n",
      "                             [-a APPEND_SOURCE] [-s RETRIEVE_SLEEP_SECONDS]\n",
      "                             [-b BASE_URL] [-cat_n CATEGORY_CLASS_NAME]\n",
      "                             [-cat CATEGORIES [CATEGORIES ...]]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -f EXPORT_FILEPATH, --export_filepath EXPORT_FILEPATH\n",
      "                        csv filepath for the output of data\n",
      "  -d DRIVER_EXE_PATH, --driver_exe_path DRIVER_EXE_PATH\n",
      "                        path to where stored chrome driver exe\n",
      "  -ret RETRIEVE_BUTTON_ID, --retrieve_button_id RETRIEVE_BUTTON_ID\n",
      "                        the id of the retrieve button in the html source\n",
      "  -down DOWNLOAD_BUTTON_ID, --download_button_id DOWNLOAD_BUTTON_ID\n",
      "                        the id of the download button in the html source\n",
      "  -v VISIBLE_BROWSER, --visible_browser VISIBLE_BROWSER\n",
      "                        whether a browser window should pop up and perform the\n",
      "                        scripted actions. Set to False for headerless\n",
      "  -a APPEND_SOURCE, --append_source APPEND_SOURCE\n",
      "                        whether the url and the pipeline name should be\n",
      "                        appended to the entries to show where entry was\n",
      "                        fetched from\n",
      "  -s RETRIEVE_SLEEP_SECONDS, --retrieve_sleep_seconds RETRIEVE_SLEEP_SECONDS\n",
      "                        how long the program will sleep after activating the\n",
      "                        retrieve button (to help if takes long time to buffer)\n",
      "  -b BASE_URL, --base_url BASE_URL\n",
      "                        what webpage to start from\n",
      "  -cat_n CATEGORY_CLASS_NAME, --category_class_name CATEGORY_CLASS_NAME\n",
      "                        the class name from the html source to which signal\n",
      "                        which tags to search for in finding categories\n",
      "  -cat CATEGORIES [CATEGORIES ...], --categories CATEGORIES [CATEGORIES ...]\n",
      "                        the pipelines to pull data from (listed in the\n",
      "                        dropdown tabs of webpage). Currently only supports one\n",
      "                        pipeline input specified with str\n",
      "arguments = Namespace(append_source=True, base_url='https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=ARLS', categories='Interstate', category_class_name='igdm_NautilusMenuItemHorizontalRootLink', download_button_id='WebSplitter1_tmpl1_ContentPlaceHolder1_HeaderBTN1_btnDownload', driver_exe_path='chromedriver_win32', export_filepath='download.csv', retrieve_button_id='WebSplitter1_tmpl1_ContentPlaceHolder1_HeaderBTN1_btnRetrieve', retrieve_sleep_seconds=2, visible_browser=True)\n",
      "# of links in Interstate = 20\n",
      "Inferred path to driver = C:\\Users\\celii\\Downloads\\chromedriver_win32\\chromedriver.exe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\celii\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:59: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Working on downloading Interstate:Arlington Storage from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=ARLS\n",
      "\n",
      "-- Working on downloading Interstate:Cheyenne Plains from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=CPD\n",
      "\n",
      "-- Working on downloading Interstate:Colorado Interstate Gas from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=CIGD\n",
      "\n",
      "-- Working on downloading Interstate:Elba Express from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=EEC\n",
      "\n",
      "-- Working on downloading Interstate:El Paso Natural Gas from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=EPGD\n",
      "\n",
      "-- Working on downloading Interstate:Horizon Pipeline from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=HORZ\n",
      "\n",
      "-- Working on downloading Interstate:KM Illinois Pipeline from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=KMIL\n",
      "\n",
      "-- Working on downloading Interstate:KM Louisiana Pipeline from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=KMLP\n",
      "\n",
      "-- Working on downloading Interstate:Midcontinent Express from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=MEP\n",
      "\n",
      "-- Working on downloading Interstate:Mojave Pipeline from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=MOJD\n",
      "\n",
      "-- Working on downloading Interstate:Natural Gas Pipeline from \n",
      " https://pipeline2.kindermorgan.com/LocationDataDownload/LocDataDwnld.aspx?code=NGPL\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-a4320594a3ab>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"arguments = {arguments}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdata_fetch_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-9b1e4aa17341>\u001b[0m in \u001b[0;36mdata_fetch_pipeline\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdata_fetch_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdata_links\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_download_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_links\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mexport_data_df_to_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-231eac0b3b75>\u001b[0m in \u001b[0;36mdownload_data\u001b[1;34m(data_to_download, driver_exe_path, retrieve_button_id, download_button_id, visible_browser, append_source, retrieve_sleep_seconds, ignore_empty_download, debug, verbose, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretrieve_sleep_seconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mdownloads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
